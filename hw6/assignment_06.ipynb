{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load point data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAI4CAYAAABndZP2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAx40lEQVR4nO3de7AkZ3nf8d+zZ4VIDAZdiBCSDoIKEGO7KsBE+Di2o4pkbiEoBoxlJ94FYcl2TDlU2bG1UeRQ2agW2zERBCe2ZBBagrkZMCrAwZLMGrtqBJylxFVg1gaEFCGZ5eoQXY705I/ukWZn59I9/Xb3+779/VRtnbNz5sz0nNnd/u3zPO/b5u4CAADIya6+DwAAACA0Ag4AAMgOAQcAAGSHgAMAALJDwAEAANkh4AAAgOwQcADUYmZvMrP/0vdxrGJmf2Jme/s+DgD9IOAAaI2ZHTKzn+vjud39ue5+bZX79nmcANpBwAEAANkh4ABYysyeZmYfN7PvmNnbJT186msnmdn7zOxvzewb5ednll+7QtKPSnq9mf2dmb2+vP21ZvYVM/u2mR02sx9d8txvMrPfM7Pry+f/czN7/NTXf9jMPmZm3yo//vDU1x6sypjZS83sL83sv5bH+UUze+6i47TCfzOzu8rj/JSZ/UDQHyyAVhFwACxkZg+T9MeS3izpZEnvlPSiqbvsknSNpMdL2pT0/yS9XpLc/TJJfyHpFe7+CHd/Rfk9H5P0j8vH+0NJ7zSzh2uxfy1pv6RTJd0s6S3lsZ0s6f2SXifpFEmvkfR+MztlweM8U9Lny8f5LUlvMDNbcJzPkvRjkp4s6VGSXiLp6JJjBBAZAg6AZX5I0gmSrnT3+9z9j1QEFEmSux9193e5+3fd/TuSrpD0z5Y9oLv/r/L7dtz9dySdKOkpS77l/e7+YXe/R9JlkrbM7CxJ/0LSF9z9zeVjvVXS5yT9ywWP82V3v9rd75d0raTTJZ224L73SXqkpH8kydz9Fne/Y9nrAhAXAg6AZR4n6XY/9qq8X558YmZ/38x+38y+bGbflvRhSY82s41FD2hmv2pmt5RtpW+qqJCcuuQYvjL5xN3/TtLXy+N63PSxTB3bGQse56tTj/Pd8tNHzLuju/+ZikrU70q6y8yuMrPvXXKMACJDwAGwzB2SzjAzm7ptc+rzX1FRfXmmu3+viraOJE3uPx2MVM7b/JqKls9J7v5oSd+auv88Z019/yNUtLb+T/nr8TP33ZR0+8pXdTw/7gb317n7MyQ9VUWr6t+v8bgAekLAAbDMWNKOpF82sxPM7IWSzpn6+iNVzN18s5yJ+U8z33+npCfO3H9H0t9K2m1mvyFpVWXkeWb2I+U80H5JN7n7VyR9QNKTzexnzGy3mf2UijDyvjVe5zHHaWb/xMyeaWYnSPq/ku6W9MAajwugJwQcAAu5+72SXijppSpaQz8l6d1Td7lS0t+T9DVJN0n63zMP8VpJLy5XLr1O0gfL+/yVinbS3ZpqQS3whyqC09clPUPSvymP7aik56uoIh1VURl6vrt/rf4rPe44v1fS1ZK+UR7nUUm/vcbjAuiJHdtaB4B4mNmbJN3m7v+x72MBkBYqOAAAIDsEHAAAkB1aVAAAIDtUcAAAQHZ2930A00499VQ/++yz+z4MAACQiMOHD3/N3R8ze3tUAefss8/W9vZ234cBAAASYWazO5pLokUFAAAyRMABAADZIeAAAIDsEHAAAEB2CDgAACA7BBwAAJAdAg4AAMgOAQcAAGSHgAMAALJDwAEAANkh4AAAgOwQcAAAQHYIOAAAIDsEHAAAkB0CDgAAyA4BBwAAZIeAAwAAskPAAQAA2SHgAACA7BBwAABAdgg4AABkbjyWDhwoPg7F7r4PAAAAtGc8ls47T7r3XulhD5NuvFHa2ur7qNpHBQcAgIwdOlSEm/vvLz4eOtT3EXWDgAMAQMbOPbeo3GxsFB/PPbfvI+oGLSoAADK2tVW0pQ4dKsLNENpTEgEHAIDsbW0NJ9hM0KICAADZIeAAAIDsEHAAAEB2CDgAAGChVDcJZMgYAADMlfImgVRwAADAXClvEkjAAQAAc6W8SSAtKgAAMFfKmwQScAAAwEKpbhJIiwoAAGSHgAMAALJDwAEAANkh4AAAgOwQcAAAQC0p7G7MKioAAFBZKrsbU8EBAACVpbK7MQEHAABUlsruxrSoAABAZansbkzAAQAAtaSwuzEtKgAABiyFFVHroIIDAMBApbIiah1UcAAAGKhUVkStg4ADAMBAzVsRVadlFXN7ixYVAAADNbsiSqresoq9vUUFBwCAAdvakvbtKz7WaVnF3t4i4AAAAEn1NvGLfcM/WlQAAEBSvU38Yt/wz9y972N40Gg08u3t7b4PAwAAJMLMDrv7aPZ2WlQAAEQk5pVJi8R4zLSoAACIROwrk+aJ9Zip4AAAEIkuViaFrrbEupqKCg4AAJGYrEyaVENCr0xqo9rS9jGvi4ADAEAk2l6ZNK/a0vQ5Yl1NRcABACAiW1vthYTpasvGhnTrrUVVJ0TIiSXYTDCDAwDAQEyqLRdfLJlJV19dtKxiWv0UCgEHAIAB2dqSNjelnZ34BoNDIuAAADAwsV9mIQRmcAAAGJhYB4NDIuAAADBAMQ4Gh0SLCgAAZIeAAwAAjjO743GdHZBjuDYVLSoAAHCM2R2Pr7xSeuUrq+2AHMu1qajgAACAY8zuePyud1W/3lQs16Yi4AAAgGPaSrPLyF/0ourLymNZgk6LCgCAgZvXVppdRv6DP1htWXksS9AJOAAADNy8ttK+fceGkzrLymNYgk6LCgCAgYulrRQSFRwAAAYulrZSSAQcAEASxuO8TsCxiaGtFBIBBwAQvVj2VokV4e94BBwAQPTmDcFyIi8Q/uZjyBgAEL0ch2BDiWVjvdhQwQEARC/HIdhQJuHv3nuLAHjrrUVVZ+g/I3P3vo/hQaPRyLe3t/s+DAAAkjIeSwcPStdcI+3sDKtVZWaH3X00ezstKgAAEre1JW1uFuFmWasqhqt8d6Vxi8rMzpJ0UNJpklzSVe7+WjM7WdLbJZ0t6UuSXuLu32j6fAAAxCaGVUzTrap5c0pDG0YOMYOzI+lX3P3jZvZISYfN7HpJL5V0o7u/2swulXSppF8P8HwAAEQjluCwak5paCvRGgccd79D0h3l598xs1sknSHpAknnlne7VtIhEXAAAJmJKTgs26xvVYUnN0FXUZnZ2ZKeJukjkk4rw48kfVVFC2ve91wi6RJJ2tzcDHk4AAC0rq3gMGl7nXKKdPRo8/bX0FaiBVtFZWaPkPTnkq5w93eb2Tfd/dFTX/+Gu5+07DFYRQUASNE6MzjLvmfS9rrnHumBB6Rdu6QTT6ze/godjmK2aBVVkAqOmZ0g6V2S3uLu7y5vvtPMTnf3O8zsdEl3hXguAABiM2kNTVYprQoUq+Z2Jm2vBx4ofv/AA9XbX03DUS4aLxM3M5P0Bkm3uPtrpr50naS95ed7Jb236XMBABCrSbC4/PLi47Kl2Kt2H560vXaVZ+ldu6q3v5aFoyEJUcH5p5J+VtKnzOzm8rb/IOnVkt5hZi+X9GVJLwnwXACABMWwjLptdYaNV83tTM/L1G0zTR57uoIzhKHiWSFWUf2lJFvw5fOaPj4AIG1Nl1GnEo7qDBvPG/idfZ3LVkQt0yQc5YRrUQEAWtVkGXUse8xUUXeV0nSACf061w1HOSHgAABa1WQZdUx7zFSxbrBI7XWmgIADAGhVk/1XhrI53VBeZ5e4mjgAIGqpzOA0NZTXGdqifXAIOAAAIFmLAk7jfXAAAED7JpsILttfBw9hBgcAgMiltJosFlRwAACI3Kqdj3E8Ag4AAJGbrLLa2GCVVVW0qAAAiNzs7sSTCg5tqsUIOAAAJGASZpjFqYYWFQCgM6wEaoZZnOqo4AAAOsFKoObY8bg6Ag4AoBNcb6m5Jpe9GBoCDgCgE1QfwujzSuEpXU6CgAMA6ATVh7Sl1mIk4AAAOtNn9aEPKVU8Vlmnxdjn6yfgAADQgtQqHqvUbTH2/fpZJg4AQA1Vl7rntqR70mLcv79aWOn79VPBAQCggvFYOnhQuuYaaWdndVUix6HqOi3Gvl8/AQcAgBUm7Za775bci9tWzaEMfai679dPwAEAYIVJu2USbsyqVSWGNlQ9q8/XzwwOAAArzF7N++d/Pv2h4dxRwQEAYIW+2y2oj4ADABi0qnu1DL3dlBoCDgCgV31uBtf3Xi1oDwEHANCbvgMGFwDNF0PGAIDeNN0Mruqme4vMDg+3vVfLeCz94i8Wv9Y9ZlRDBQcA0Jsmm8GFqP50OTw8HhfPce+9xe+vuUb60IeoGLWFgAMA6E2TgBGqvdTV8PChQ9J99z30e1pi7SLgAAB6tW7AaPtSAMuGn9cZjD73XOmEEx6q4ORy+YZYEXAAALX0ueppWpvtpWXtr3VbY1tbxbEePFj8fs8eqjdtIuAAACoLveqpaVhqq720rP01/bV77pFe9ariV9WQQ6jpBquoAACVNV31NG0Sli6/vPgY06qiZaurJl/btUt64AHphhviO/6+NF3VFhIBBwBQWchl1SHDUmiT9tf+/cdXqSZfO//8h0JObMffh9gCKy0qAEBlVedeqrSe2h4SbmpZO2lrq2hL/cVfND/+Jm26WOahpPg2TSTgAABqWTVHUnVOJ7YLWNYNC8uOv+pjNZlp6nsX6FmxBVYCDgAgqDr/k49l6LbJyqh5S8irPlaTqkdsFZPYAiszOACAtSwaKO368gchhJwHqvNYTX5WMf6ct7akffv6DzcSFRwAwBqWVSli+5/8ItNtpJDtlTqP1eRnlcrPuS8EHABAbavaI222nkIM1s4LaKHCQt3g0eRnFUuLL0YEHADIRJcravoaKA01WDsvoIVsrRA8+kfAAYAMdL2ipq/2SKjB2thW/CA8Ag4AZKCPFTV9VClCBRPmV/JHwAGADAylIlE3mCxq28W0QR7aQcABgAwMqSJRpXI0HhdX7b7mGmln59i2XWwb5KEdBBwAyASDrYVJgLn7bsm9uG26bVe3nUe1J00EHAAYsBxP3pMAMwk3Zse27eq086j2pIuAAwADlevJezrAbGxIF10k7dmz3kaEsV0OAdURcABgoFadvFOt7lQJMFXbebEOb6f63nSJgAMAA7Xs5N1ndSfEybvJPNLs88c2vJ1r5S00Ag4AZKhKSFh28u6rNdP05N00HC16/pgCBG2zagg4AJCZOiFh0cm7r9ZMk5N3iMpGCuEh1rZZbAg4AJCZECfpvlozTU7eIV53CuEhxrZZjAg4AJCZkJczqHLyDDnw2uTkHeJ1pxIeYmubxch8slFABEajkW9vb/d9GACQvK5W2cQ28MrqouExs8PuPpq9nQoOAGSoq//hxzazQmUDE7v6PgAAQLombaGNjXhnVjBMVHAAAGtLZWYFw0PAAQA0sk5bqK9ZmaE975ARcAAAneprMHlozzt0zOAAADo1bzCZ50VoBBwAgMZj6cCB4mPbqg4mrzqmusfc10A0g9j9oEUFAAMXooVSZ8akymDyqmNa55j7GohmELsfBBwAGLiqe9ksCjHrho1l91l1TOvuv9PXPjnsz9M9Ag4ADFyVSxwsCzHLwsa6q4dWHVMK14xCvwg4ADBwVVooy0LMorDRpPW16pho+2AVAg4AYGULZVnFZFHYaHoZh1XHRNsHyxBwAAArVamozN5GGwl9IuAAACqpWzGhjYQ+EXAAAK2hjYS+sNEfACCYLjcMBJahggMACKLPay6tWo7OxS6Hh4ADAAii6aqpdbWx6zHSR4sKABBEX9dcWnUxSy52OUxUcAAAQfS1aopdjzGPuXvfx/Cg0Wjk29vbfR8GAGBKCvMrzOAMl5kddvfRcbcTcAAAi3Q5v0IIwToWBRxaVACAhboaHGYQGKExZAwAWKirwWEGgREaFRwASEQfLZyuBocZBEZoBBwASECfLZwuLrfAdasQGgEHABLQ1yZ6XeK6VQiJGRwASEBfm+gBqaKCAwAJoIUD1EPAAYBE0MIBqqNFBQAdGI+lAweKjyk9NpAqKjgA0LI2V0CxQR4wHxUcAGhZm5vYsUEeMB8BBwBa1uYKqHUfe15bi1YXckKLCgBa1uYKqHUee15bS6LVhbwECThm9kZJz5d0l7v/QHnbyZLeLulsSV+S9BJ3/0aI5wOA1LS5AmrZY8+7vMOitlbuGwliWEJVcN4k6fWSDk7ddqmkG9391WZ2afn7Xw/0fACAOaYDjTS/KrPouk9cCwo5CRJw3P3DZnb2zM0XSDq3/PxaSYdEwAGA1sy2nvbunV+VWdTWYiNB5KTNGZzT3P2O8vOvSjpt3p3M7BJJl0jS5uZmi4cDAHmbbT1Ji6sy89pabCSInHSyisrdXZIv+NpV7j5y99FjHvOYLg4HALI0u6Jqz56iKrN/P0PDGJ42Kzh3mtnp7n6HmZ0u6a4WnwsAsjNvQHiZRa0ngg2GqM2Ac52kvZJeXX58b4vPBQBZWXeHYtpMQCFIi8rM3ippLOkpZnabmb1cRbD5cTP7gqTzy98DACpgh2KgmVCrqH56wZfOC/H4ANJQt6WCxRYt5QZQDTsZAwiCiz6G1ebux8AQEHAABDGvpcJJuRnmaYD1cbFNAEG0eUFJAKiLCg6AIGip1NP1vBLzURgaAg6AYGipVNP1vBLzURgiWlQAsjEeSwcOFB/7fIxVul4CzpJzDBEVHABZCFGl6KrS0fUScJacY4gIOACyEGIVV1crwbqeV2I+CkNEwAGQhRBVii4rHYvmldoaBmY+CkNDwAGQhRBVir4rHQwDA+EQcABkI0SVos9KB5slAuGwigoAIsFmiUA4VHAAZC2lDe76bpEBOSHgAMhWijMtDAMDYdCiApCttje462JTQADroYIDIFttLvtOsToEDAkBB0C22pxpYcUTEDcCDoCstTXTwuUPgLgRcABkqe3VU+tWh1Ja1QWkjIADIDtdzcfUrQ4xtwN0h1VUALLT9uqpOqZXWsV0XEDuqOAAyE4s8zGzFZsrr4zjuIAhIOAAyE4sOwLPVmyOHo3juIAhIOAAyFKfOwJP2lGnnHJ8xYadioFuEHAADFroVU3z2lJHj1KxAbpGwAEwWG2saprXltq3L8jhAqiBVVQABquNVU2TAeeNDQaJgT5RwQEwWG2stoplwHkRNhrEUBBwAAxW1TBSNxTEOkjMRoMYEgIOgEFbFUZyCgVcIBRDwgwOACwR8+7D07skV8F8EIaECg4ALDAeS7feKu0u/6WMKRSsU1mKfT4ICImAAwBzTAeIjQ3p4oulPXviCQXrtptinQ8CQqNFBQBzTAeI+++XNjfjCga0m4DlqOAAwByxXLBzkTbbTSwlRw4IOAAwRwrzKm20m3JaNYZhI+AAwAJDnFdhKTlywQwOANRUZXl23SXcsWC2B7mgggMANVRp4aTc5kmhNQdUQcABgBqqtHBSb/MMsTWH/NCiAoAaqrRwaPMA/aOCAwA1zGvhzC6rXqfNw9JsICxz976P4UGj0ci3t7f7PgwAqCzEvE3KMztA38zssLuPZm+nRQUADYS4GGfMF/QEUkXAARBcqkuk1xFi3oaZHSA8ZnAABNVluyWGuZUQy6pZmg2ER8ABAorhhNu3LpZIj8fSwYPSNddIOzv9z62EWFbN0mwgLAIOEAiDooW2L1I5+Tnffbc0WSMxG6QImgAIOEAgqW/uNmvdkNB2u2Xyc56EG7NjgxRBE4BEwAGCabty0aWmIaHNdsv0z3ljQ7roImnPnoeeL7egCWA9BBwgkJwGRWMOCat+zqkFTdppQDsIOEBAuQyKxh4Slv2cUwqatNOA9hBwABwnpZAwzzpBs49KStVKGVUeoD4CDoC5cqlGVdFXJaVKpYwqD7AedjIGBi6lXYfbOta+LpUwqZTt3784uHAZB2A9VHCAAUupOtDmsfY5c7SqUhb7PBQQKwIOMGChVkt1MSPS5squmGeOYj42IGYEHGDAQlQHuqoCtV3JmK6kxDbUO6R5KCAUAg4wYCGqA+tWVuqGiK4qGSm17QAsRsABBq5pdWCdysq6IaKLSkbMmxwCqI6AA6CRdSorMYcIhnqBPBBwADRWp7IyHku33irtLv/1iS1EMNQL5IGAA6Az062pjQ3p4oulpz3tob1dYgkTDPUC6SPgAOjMdGtq4pWvTGegN7bVVQAWI+AA6MzsfIsU7yzOLFZXAWkh4ADozOx8iyRde20aA70xD0YDOB4BB0CnZudbZgd6Y20DsboKSAsBB0CvZncQjrUNxOoqIC0EHCAjsVY/qoq9DcTqKiAdBBwgEzFXP6qiDVRIPagCMSDgAJmIvfpRReptoBDBJIegCsSAgANkIpfqR6ptoFDBpEpQpcIDrEbAATKRevUjdaEqaKuCKhUeoBoCDtChtv/nnWr1Y5GUKhWhKmirgmoOrUigCwQcoCP8z7ue1H5eIStoy4JqLq1IoG0EHKAj/M+7nhR/Xl1U0GhFAtUQcICO8D/vevh5LZZbKxJoAwEH6Aj/866HnxeAJszd+z6GB41GI9/e3u77MACskNLwL4C8mdlhdx/N3k4FB+hAToEgteFfAMNEwAFallsgaGv4N6cQCKB/BBygZSmuBlqmjeHf3EIggP4RcICW5bYaqI3h39xCIID+EXCAluW4Gij0MuXcQiCA/hFwgA6ksG/JOjMwoeZm5oVAZnIANEHAAbDWDEzouZnpENjGTA6BCRiWXX0fAID+zZuBCf0947F04EDxsY3jWfXc550nXX558bHKMQBIGxUcoGMxVhLWmYGp8z11KzKhZ3IYYgaGh4ADdCjW5dDrDELX+Z66ASP0YDZDzMDwEHCADsVcSVhnELrq96wTMOocz6qqWI4r2QAsR8ABOjTUSkKbAaNqVWwSmCazQAQdIG8EHKBDXS2HjnHOp62l8nWqYrG2CAGE13rAMbPnSHqtpA1Jf+Dur277OYGYrbscumpoGdpJvE5VLOYWIYCwWg04ZrYh6Xcl/bik2yR9zMyuc/fPtvm8QCqqnnDrhJahncTrtL+G2iIEhqjtCs45ko64+99Ikpm9TdIFkgg4gKqfcOuEliGexKu2vxg2Boaj7YBzhqSvTP3+NknPnL6DmV0i6RJJ2tzcbPlwgLhUPeHWCS2cxJdL4bIZAJozd2/vwc1eLOk57v5z5e9/VtIz3f0V8+4/Go18e3u7teMBUhbj4DAA9M3MDrv7aPb2tis4t0s6a+r3Z5a3AaiJygMAVNd2wPmYpCeZ2RNUBJsLJf1My88JYI51K0DT3ydRRQKQhlYDjrvvmNkrJH1QxTLxN7r7Z9p8TgDHW3fp+PT3bWxIZtLOTvfLz2nPAair9X1w3P0Dkj7Q9vMAWOzgQenuuyX3ekvHp1dvPfBAcVvdx2hqaPv6AAhjV98HAKBd47H0xjcWwUSSdu8uKiGTSxaMx4u/d7J6a2NDOuGEhz7vcvn5vCXyALAKl2oAMnfoUBEOpKLF9LKXFZ9XvX7T9JLzyeN12Soa4r4+AJoj4ACRazp/MhsQ9uypt3Hg7OqtrttD7OsDYB0EHCBiIeZPFgWELqoioYaDWSIPoC4CDhCxUNeVmleFabsqEvtwMCuzgLwRcICeVDnBtjl/0nZVJOaLfsYevgA0R8BBlmL/33nVE2zK8ycxDwfHHL4AhEHAQXaabGrXVZBoMuSbipjDWczhC0AYBBxkp2p4mL0EQRsti0WhaSgn2MlrnuxdE0vIiTl8AQiDgIPsVAkPs1WevXvDtyyWVZKGcoKNedYl1coYgGoIOMhOlfAwW+WRwldUVlWShnCCZdYFQF8IOMjSqvAwb/O7yQZ4oSoqddtQsQ9Gr2MorTgA8TGfXKAmAqPRyLe3t/s+DAxEF4Gi6nNUbeWkGIJSPGYA6TCzw+4+mr2dCg4GYd5JtosWUdXnqNLKiXmeZZkhtOIAxIeAg+ylEAyqtHKYZwGA6nb1fQBA2+YFg9hMBqP3718cwCYhaGOj+HXrrUV4AwAcj4CDVo3H0oED/Z6Ip4NBzIOuW1vSvn3LN/y78Ubp4oslM+nqq4vKFCEHAI5HiwqtiaU1lNOeM1tbxevY2aFVBQDLEHDQmphmRuoMulZZ9dPnyiCWXgPAagQctCbFE3GVqlPflamcKlIA0BYCDlqT4om4StUphsoUS68BYDkCDlrVxom4zfZQlapTipUpABgaAg6S0nZ7qErVKcXKFAAMDQEHSemiPVSl6pRzi4hLKwDIAQEHSaE9VE/dsNL3ADUAhELAQVJm20NSsZFgl9WGVCoc64SVGAaoASAEAg6SM2kP9VFtWPScMYaedcIKFTIAuSDgIFl9VBsWXdcqxrbOOmGFAWoAuSDgIFldVxvG4+ICl7vLvzWT54y1rbNuWMl5gBrAcBBwkKwuqw3TramNjeKCl3v2PPScsbZ1CCsAhoqAg6R1dQKfrtJI0ubmQ89LWwcA4kPAASpY1Q6jUgIAcSHgABXEWqWJcfUWAMSAgANUFFuVhk35AGCxXX0fAID1LFqyDgAg4ACNjMfFTsrjcffPPZkL2tiIb/UWAPSNFhWwpr5bRLHOBQFADAg4wJpi2OAvtrkgAIgFLSpgTbSIACBeVHCANdEiAoB4EXCABmgRAUCcaFEBAIDsEHAAAEB2CDgAACA7BBwMSp8b8wEAusOQMQaj7435AADdoYKDweDaTQAwHAQcDAYb8wHAcNCiwmCwMR8ADAcBB0GMx2kEh1w35qvz80/lvQKAJgg4aIzh3X7V+fnzXgEYCmZw0BjDu/2q8/PnvQIwFAQcNJb68G7qe+PU+fmn/l4BQFW0qNBYasO70zMoUvotmzo//9TeKwBYFwEHQfQ9vFt1cHZ2BmXv3uNbNime9Ov8/Pt+rwCgCwQcJK/O4OzsDIpUfM/ke2nZAEAemMEZmNTnTeapMzg7O4OyZ08RiPbvj689leN7BQBdoYIzILkuEZ6ElipVmEUzKLH9HHJ9rwCgKwScAZlX6cjhpFl3cDaFGZTY3is2BwSQGgLOgNSpdPShyUk0hdBSR0zvFdUkACki4AxIzEuEczuJNq14xPRexVZNAoAqCDgDE2ulI+aTaN2wEiqsxfJexVRNAoCqCDiIQqwn0XXCSsxhbR0xVZMAoCoCzkDFNjQa60l0nbASa1hrIpZqEgBURcAZoFjnXWI8ia4TVmINawAwJAScAcqthbJMX8O+MYY1ABgSAs4A5dhCmSe3YV8AQHVcqmGAJlWJPi9PUOcyBOtesqDOJRwAAHmhgjNQfVYl6lRWmlRhhlKpAgAcjwpOBlK7KGOdykqTKkwMlSoAQD+o4CQu1hVRy9SprDStwjA/AwDDRMBJXIoroqquTJqsgLrySuno0eEsuY5tjyIASBEBJ3GpzpmsqqykWJkKYaivGwBCYwYncW3MmcQw0xPDCqg+fg4xvG4AyAEVnAxMV0OatjdiqSD0XZnq6+fQ9+sGgFwQcDIS4qQcy0xP35c76Ovn0PfrBoBcEHAyEuKkHFMFoc8VUH3+HFj5BQDNEXAyEuKkTAWhwM8BANJm7t73MTxoNBr59vZ234eRtC6WGC97DpY4AwC6ZGaH3X00ezsVnMy03d5YNucTy4AyAAAsE0cty5Yxs8QZABALAg5qmcz5bGwcP+ez7GsAAHSJFhVqWTZ8y2AuACAWDBkDAIBkLRoypkUFAACyQ8ABAADZIeAkKIaLYfZhqK8bAFAfQ8aJGepeM0N93QCA9VDBScxQ95oZ6usGAKyHgJOYNveaibkFxB47AIA6aFElpq29ZmJvAbHHDgCgjkYBx8x+UtKrJH2fpHPcfXvqa/skvVzS/ZJ+2d0/2OS5Ytb1BSbbuN7UvBZQVyGi6s+v7etsAQDy0bSC82lJL5T0+9M3mtlTJV0o6fslPU7SDWb2ZHe/v+HzRSf2ykdVkxbQ5HV01QLK5ecHAIhLoxkcd7/F3T8/50sXSHqbu9/j7l+UdETSOU2eK1a5DL9OWkD793cbMnL5+QEA4tLWDM4Zkm6a+v1t5W1ZmG6p9FX5CGW2PdR19ST1nx8AIE4rA46Z3SDpsXO+dJm7v7fpAZjZJZIukaTNzc2mD9e6eS2VVIdfQ7SHms4fMTwMAGjDyoDj7uev8bi3Szpr6vdnlrfNe/yrJF0lFRfbXOO5OjWvpbJvX5on5qaDxaHmZxgeBgCE1tY+ONdJutDMTjSzJ0h6kqSPtvRcncppP5amr2XZ/EzMe+oAAPLXdJn4T0j675IeI+n9Znazuz/b3T9jZu+Q9FlJO5J+KZcVVDm1VJq+lkXzM8sqO10vqQcADFOjgOPu75H0ngVfu0LSFU0eP1Y5tVSavJZFAWlR64sl4QCArrCTMRqZF5AWVXb63EwQADAsBBwEt6iyw5JwAEBXCDhoxbzKTk7zSwCAuBFwKmAwNpyc5pcAAPEi4KzAYGxaCKMAAImAs1JMg7F1Tt5DPNETRgEAEwScFWIZjK1z8o7xRN9F4IopjAIA+kXAWSGWwdg6J+/YTvRdBa5YwigAoH8EnApiGIytc/KO7UTfVeCKJYwCAPpHwElEnZN3bCf6LgNXDGEUANA/c4/nAt6j0ci3t7f7Pgy0YIhDzwCA9pnZYXcfzd5OBQedoLICAOjSrr4PAAAAIDQCDgAAyA4BBwAAZIeAk7nxWDpwoPgY02MBANAmhox71ubqopAb7MW4OzIAAItQwenRJDRcfnnxMXRlZN4GezE81gQVIQBAW6jgNNC0+tL2Dr8hN9gLvVkfFSEAQJsIOGsKcYJue4ffkDsah94dObbrZQEA8kLAqWG6YhPiBN3FJRVCbrAX8rFiu14WACAvBJyKZis2V14Z5gQ91B1+Y7teFgAgLwScimYrNkePcoJuaqjhDgDQvsEEnKYDwfNaKpygAQCI0yACToiB4CG3VLgSOAAgNYMIOKFW7KRcsVk3pPSxnJtABQBoahABZ+grdlaFlGWBouvl3OyPAwAIYRABZ8jtJWl5SFkVKLoOh+yPAwAIYRABR4q/vTQeSwcPFp/v2dPdjsarAkXX4XDo1TYAQBjm7n0fw4NGo5Fvb2/3fRidG4+LE/m99xa/P/FE6UMfChsmFrWhYmwJMYMDAKjKzA67+2j29sFUcGJ26JB0330P/b6N1syiClaM7bsq1TZCEABgGQJOx+admM89VzrhhIcqOFVbM6FO8rG372bFWHUCAMSFgFPqoiKw6MS8tVU8d50ZnCGf5BlEBgCsQsBRd2Fh2Ym5bhVlyCd5BpEBAKsQcNRdWAh5Yh7yST7GuSEAQFwIOOouLIQ8MQ/9JJ/a3BAAoFssEy+xKgcAgPSwTHwFKgIAAORjV98HkLvxWDpwoPgIAAC6QQWnRUNeyg0AQJ+o4LRo3uqsdVAFAgCgHio4LQqxOosqEAAA9VHBadFkKff+/esHk1BVoDZQWQIAxIoKTssWrc6quiw91g39qCwBAGJGwOlBnXAQ64Z+Q75UBAAgfgScGV1s+Fc3HMS4R0+slSUAACQCzjG6arvkEA5irSwBACARcI7RVdsll3AQY2UJAACJgHOMppWVOu0twgEAAO0h4ExpUllhVREAAPEg4MxYt7KS46oirrAOAEgVASeQHAaHp1GRAgCkjIATSC6DwxM5VqQAAMNBwAkop8Hh3CpSAIBhIeBgrtwqUgCAYSHgYKGcKlIAgGHhauKBcYVtAAD6N/gKTsil0Kw8AgAgDoMOOKEDCSuPAACIw6BbVPMCSROTlUcbG6w8AgCgT4Ou4IReCs3KIwAA4mDu3vcxPGg0Gvn29nanz9nW5Qi4zAEAAO0zs8PuPpq9fdAVHKmdpdAMGwMA0K9Bz+C0JfRsDwAAqIeA0wKGjQEA6NfgW1Rt6GPYmJkfAAAeQsBpSZeXOWDmBwCAY9GiilDdyz0w8wMAwLGo4ERmnWpM6P18AABIHQEnMutc7oENBgEAOBYBJzLrVmO6nPkBACB2BJzIUI0BAKA5Ak6EqMYAANAMq6gAAEB2CDhz1F2mDQAA4kKLagab5gEAkD4qODPYNA8AgPQRcGZwoUwAANJHi2oGy7QBAEgfAWcOlmkDAJA2WlRLsJoKAIA0UcFZgNVUAACkiwrOAqymAgAgXQScBVhNBQBAugbbohqPl6+UYjUVAADpGmTAqTpfw2oqAADSNMgWFfM1AADkbZABh/kaAADyNsgWFfM1AADkbZABR2K+BgCAnA2uRcXuxAAA5G9QFRx2JwYAYBgaVXDM7LfN7HNm9kkze4+ZPXrqa/vM7IiZfd7Mnt34SANIbfUU1SYAANbTtIJzvaR97r5jZr8paZ+kXzezp0q6UNL3S3qcpBvM7Mnufn/D52tksnpqUsGJefUU1SYAANbXqILj7n/q7jvlb2+SdGb5+QWS3ubu97j7FyUdkXROk+cKYbJ6av/++ANDatUmAABiEnIG5yJJby8/P0NF4Jm4rbztOGZ2iaRLJGlzczPg4cyXyuqplKpNAADEZmXAMbMbJD12zpcuc/f3lve5TNKOpLfUPQB3v0rSVZI0Go287vfnir16AABY38qA4+7nL/u6mb1U0vMlnefuk4Byu6Szpu52Znkbakil2gQAQGyarqJ6jqRfk/QCd//u1Jeuk3ShmZ1oZk+Q9CRJH23yXAAAAFU1ncF5vaQTJV1vZpJ0k7v/grt/xszeIemzKlpXv9T3CioAADAcjQKOu//DJV+7QtIVTR4fAABgHYO7VAMAAMgfAQcAAGSHgNMRLrsAAEB3BnWxzb5w2QUAALpFBacDXHYBAIBuEXA6MLnswsYGl10AAKALtKg6wGUXAADoFgGnI1x2AQCA7tCiAgAA2SHgAACA7BBwAABAdgg4AAAgOwQcAACQHQIOAADIDgEHAABkh4ADAACyQ8ABAADZIeAAAIDsEHAAAEB2CDgAACA7BBwAAJAdAg4AAMgOAQcAAGSHgAMAALJDwAEAANkh4AAAgOwQcAAAQHYIOAAAIDvm7n0fw4PM7G8lfbnFpzhV0tdafPzY8HrzNqTXO6TXKvF6c8frDevx7v6Y2RujCjhtM7Ntdx/1fRxd4fXmbUivd0ivVeL15o7X2w1aVAAAIDsEHAAAkJ2hBZyr+j6AjvF68zak1zuk1yrxenPH6+3AoGZwAADAMAytggMAAAaAgAMAALKTXcAxs580s8+Y2QNmNpr52j4zO2JmnzezZy/4/ieY2UfK+73dzB7WzZE3Vx7vzeWvL5nZzQvu9yUz+1R5v+2ODzMYM3uVmd0+9Zqft+B+zynf8yNmdmnXxxmKmf22mX3OzD5pZu8xs0cvuF+y7++q98rMTiz/nB8p/56e3cNhBmFmZ5nZh8zss+W/Wf9uzn3ONbNvTf0Z/40+jjWUVX82rfC68v39pJk9vY/jDMHMnjL1vt1sZt82s1fO3Cfp99fM3mhmd5nZp6duO9nMrjezL5QfT1rwvXvL+3zBzPa2coDuntUvSd8n6SmSDkkaTd3+VEmfkHSipCdI+mtJG3O+/x2SLiw//z1Jv9j3a1rz5/A7kn5jwde+JOnUvo8xwGt8laRfXXGfjfK9fqKkh5V/Bp7a97Gv+XqfJWl3+flvSvrNnN7fKu+VpH8r6ffKzy+U9Pa+j7vB6z1d0tPLzx8p6a/mvN5zJb2v72MN+JqX/tmU9DxJfyLJJP2QpI/0fcyBXveGpK+q2JAum/dX0o9JerqkT0/d9luSLi0/v3Tev1OSTpb0N+XHk8rPTwp9fNlVcNz9Fnf//JwvXSDpbe5+j7t/UdIRSedM38HMTNI/l/RH5U3XSvpXLR5uK8rX8RJJb+37WCJwjqQj7v437n6vpLep+LOQHHf/U3ffKX97k6Qz+zyeFlR5ry5Q8fdSKv6enlf+eU+Ou9/h7h8vP/+OpFskndHvUfXuAkkHvXCTpEeb2el9H1QA50n6a3dvc6f+zrn7hyV9febm6b+ji86hz5Z0vbt/3d2/Iel6Sc8JfXzZBZwlzpD0lanf36bj/zE5RdI3p04i8+6Tgh+VdKe7f2HB113Sn5rZYTO7pMPjasMrylL2GxeUQqu87ym6SMX/dOdJ9f2t8l49eJ/y7+m3VPy9TVrZanuapI/M+fKWmX3CzP7EzL6/2yMLbtWfzVz/vl6oxf/hzOn9laTT3P2O8vOvSjptzn06eZ93h37ALpjZDZIeO+dLl7n7e7s+ni5VfO0/reXVmx9x99vN7B9Iut7MPlcm8egse72S/qek/Sr+0dyvoi13UXdHF16V99fMLpO0I+ktCx4mmfcXkpk9QtK7JL3S3b898+WPq2hr/F05Y/bHkp7U8SGGNLg/m+Uc5wsk7Zvz5dze32O4u5tZb3vRJBlw3P38Nb7tdklnTf3+zPK2aUdVlER3l/87nHefXq167Wa2W9ILJT1jyWPcXn68y8zeo6I1EOU/MlXfazO7WtL75nypyvsejQrv70slPV/SeV42s+c8RjLv74wq79XkPreVf9YfpeLvbZLM7AQV4eYt7v7u2a9PBx53/4CZ/Q8zO9Xdk7xQY4U/m0n9fa3ouZI+7u53zn4ht/e3dKeZne7ud5Ttxbvm3Od2FfNHE2eqmJsNakgtquskXViuwniCipT80ek7lCeMD0l6cXnTXkmpVYTOl/Q5d79t3hfN7HvM7JGTz1UMrn563n1jN9Ob/wnNfx0fk/QkK1bHPUxFqfi6Lo4vNDN7jqRfk/QCd//ugvuk/P5Wea+uU/H3Uir+nv7ZoqAXu3J26A2SbnH31yy4z2MnM0Zmdo6Kf7OTDHQV/2xeJ2lPuZrqhyR9a6rdkaqFFfWc3t8p039HF51DPyjpWWZ2Ujla8KzytrD6mLxu85eKE91tku6RdKekD0597TIVqzQ+L+m5U7d/QNLjys+fqCL4HJH0Tkkn9v2aar7+N0n6hZnbHifpA1Ov7xPlr8+oaH30ftxrvtY3S/qUpE+q+Et1+uzrLX//PBUrVP468dd7REXf+uby12Q1UTbv77z3StJ/VhHqJOnh5d/LI+Xf0yf2fcwNXuuPqGivfnLqPX2epF+Y/B2W9IryffyEisHyH+77uBu83rl/Nmder0n63fL9/5SmVsKm+EvS96gILI+aui2b91dFcLtD0n3lefflKmbibpT0BUk3SDq5vO9I0h9Mfe9F5d/jI5Je1sbxcakGAACQnSG1qAAAwEAQcAAAQHYIOAAAIDsEHAAAkB0CDgAAyA4BBwAAZIeAAwAAsvP/AREpne5krbslAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "(256,)\n",
      "(256,)\n"
     ]
    }
   ],
   "source": [
    "filename    = 'assignment_06_data.csv'\n",
    "data_load   = np.loadtxt(filename, delimiter = ',')\n",
    "\n",
    "x   = data_load[0, :]\n",
    "y   = data_load[1, :]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.plot(x, y, '.', color = 'blue')\n",
    "plt.title('data points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('-----')\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "n = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(x, y, theta0, theta1):\n",
    "    r_squared = 0\n",
    "    for i in range(256):\n",
    "        r_squared += np.square(theta0 + theta1*x[i] - y[i])\n",
    "        \n",
    "    loss = (1/2*n) * r_squared\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute the gradient for each model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_theta0(x, y, theta0, theta1):\n",
    "    theta0_temp = 0\n",
    "    for i in range(256):\n",
    "        theta0_temp += (theta0 + theta1*x[i]) - y[i]\n",
    "    dL = (1/n) * theta0_temp\n",
    "\n",
    "    return dL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_theta1(x, y, theta0, theta1):\n",
    "    theta1_temp = 0\n",
    "    for i in range(256):\n",
    "        theta1_temp += (theta0 + theta1*x[i]) - y[i]\n",
    "    dL = (1/n) * theta1_temp\n",
    "\n",
    "    return dL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient descent for each model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration =    0, loss = 4843980.97830\n",
      "iteration =    1, loss = 4702011.58768\n",
      "iteration =    2, loss = 4563703.78665\n",
      "iteration =    3, loss = 4428976.34558\n",
      "iteration =    4, loss = 4297749.73496\n",
      "iteration =    5, loss = 4169946.09072\n",
      "iteration =    6, loss = 4045489.18026\n",
      "iteration =    7, loss = 3924304.36917\n",
      "iteration =    8, loss = 3806318.58861\n",
      "iteration =    9, loss = 3691460.30332\n",
      "iteration =   10, loss = 3579659.48039\n",
      "iteration =   11, loss = 3470847.55847\n",
      "iteration =   12, loss = 3364957.41780\n",
      "iteration =   13, loss = 3261923.35069\n",
      "iteration =   14, loss = 3161681.03268\n",
      "iteration =   15, loss = 3064167.49424\n",
      "iteration =   16, loss = 2969321.09310\n",
      "iteration =   17, loss = 2877081.48703\n",
      "iteration =   18, loss = 2787389.60731\n",
      "iteration =   19, loss = 2700187.63258\n",
      "iteration =   20, loss = 2615418.96339\n",
      "iteration =   21, loss = 2533028.19708\n",
      "iteration =   22, loss = 2452961.10333\n",
      "iteration =   23, loss = 2375164.60009\n",
      "iteration =   24, loss = 2299586.73005\n",
      "iteration =   25, loss = 2226176.63759\n",
      "iteration =   26, loss = 2154884.54614\n",
      "iteration =   27, loss = 2085661.73610\n",
      "iteration =   28, loss = 2018460.52307\n",
      "iteration =   29, loss = 1953234.23664\n",
      "iteration =   30, loss = 1889937.19955\n",
      "iteration =   31, loss = 1828524.70727\n",
      "iteration =   32, loss = 1768953.00802\n",
      "iteration =   33, loss = 1711179.28318\n",
      "iteration =   34, loss = 1655161.62808\n",
      "iteration =   35, loss = 1600859.03320\n",
      "iteration =   36, loss = 1548231.36577\n",
      "iteration =   37, loss = 1497239.35166\n",
      "iteration =   38, loss = 1447844.55775\n",
      "iteration =   39, loss = 1400009.37456\n",
      "iteration =   40, loss = 1353696.99929\n",
      "iteration =   41, loss = 1308871.41920\n",
      "iteration =   42, loss = 1265497.39527\n",
      "iteration =   43, loss = 1223540.44625\n",
      "iteration =   44, loss = 1182966.83305\n",
      "iteration =   45, loss = 1143743.54335\n",
      "iteration =   46, loss = 1105838.27662\n",
      "iteration =   47, loss = 1069219.42943\n",
      "iteration =   48, loss = 1033856.08096\n",
      "iteration =   49, loss = 999717.97897\n",
      "iteration =   50, loss = 966775.52587\n",
      "iteration =   51, loss = 934999.76527\n",
      "iteration =   52, loss = 904362.36859\n",
      "iteration =   53, loss = 874835.62214\n",
      "iteration =   54, loss = 846392.41431\n",
      "iteration =   55, loss = 819006.22313\n",
      "iteration =   56, loss = 792651.10398\n",
      "iteration =   57, loss = 767301.67768\n",
      "iteration =   58, loss = 742933.11867\n",
      "iteration =   59, loss = 719521.14355\n",
      "iteration =   60, loss = 697041.99982\n",
      "iteration =   61, loss = 675472.45480\n",
      "iteration =   62, loss = 654789.78483\n",
      "iteration =   63, loss = 634971.76468\n",
      "iteration =   64, loss = 615996.65720\n",
      "iteration =   65, loss = 597843.20305\n",
      "iteration =   66, loss = 580490.61088\n",
      "iteration =   67, loss = 563918.54742\n",
      "iteration =   68, loss = 548107.12805\n",
      "iteration =   69, loss = 533036.90734\n",
      "iteration =   70, loss = 518688.86993\n",
      "iteration =   71, loss = 505044.42151\n",
      "iteration =   72, loss = 492085.38003\n",
      "iteration =   73, loss = 479793.96706\n",
      "iteration =   74, loss = 468152.79937\n",
      "iteration =   75, loss = 457144.88061\n",
      "iteration =   76, loss = 446753.59323\n",
      "iteration =   77, loss = 436962.69053\n",
      "iteration =   78, loss = 427756.28887\n",
      "iteration =   79, loss = 419118.86006\n",
      "iteration =   80, loss = 411035.22389\n",
      "iteration =   81, loss = 403490.54080\n",
      "iteration =   82, loss = 396470.30474\n",
      "iteration =   83, loss = 389960.33614\n",
      "iteration =   84, loss = 383946.77503\n",
      "iteration =   85, loss = 378416.07430\n",
      "iteration =   86, loss = 373354.99313\n",
      "iteration =   87, loss = 368750.59051\n",
      "iteration =   88, loss = 364590.21890\n",
      "iteration =   89, loss = 360861.51805\n",
      "iteration =   90, loss = 357552.40892\n",
      "iteration =   91, loss = 354651.08772\n",
      "iteration =   92, loss = 352146.02012\n",
      "iteration =   93, loss = 350025.93548\n",
      "iteration =   94, loss = 348279.82134\n",
      "iteration =   95, loss = 346896.91786\n",
      "iteration =   96, loss = 345866.71253\n",
      "iteration =   97, loss = 345178.93487\n",
      "iteration =   98, loss = 344823.55130\n",
      "iteration =   99, loss = 344790.76011\n",
      "iteration =  100, loss = 345070.98651\n",
      "iteration =  101, loss = 345654.87779\n",
      "iteration =  102, loss = 346533.29861\n",
      "iteration =  103, loss = 347697.32634\n",
      "iteration =  104, loss = 349138.24654\n",
      "iteration =  105, loss = 350847.54847\n",
      "iteration =  106, loss = 352816.92078\n",
      "iteration =  107, loss = 355038.24721\n",
      "iteration =  108, loss = 357503.60243\n",
      "iteration =  109, loss = 360205.24793\n",
      "iteration =  110, loss = 363135.62803\n",
      "iteration =  111, loss = 366287.36594\n",
      "iteration =  112, loss = 369653.25991\n",
      "iteration =  113, loss = 373226.27948\n",
      "iteration =  114, loss = 376999.56179\n",
      "iteration =  115, loss = 380966.40794\n",
      "iteration =  116, loss = 385120.27949\n",
      "iteration =  117, loss = 389454.79499\n",
      "iteration =  118, loss = 393963.72655\n",
      "iteration =  119, loss = 398640.99658\n",
      "iteration =  120, loss = 403480.67446\n",
      "iteration =  121, loss = 408476.97343\n",
      "iteration =  122, loss = 413624.24743\n",
      "iteration =  123, loss = 418916.98803\n",
      "iteration =  124, loss = 424349.82148\n",
      "iteration =  125, loss = 429917.50574\n",
      "iteration =  126, loss = 435614.92765\n",
      "iteration =  127, loss = 441437.10008\n",
      "iteration =  128, loss = 447379.15919\n",
      "iteration =  129, loss = 453436.36177\n",
      "iteration =  130, loss = 459604.08256\n",
      "iteration =  131, loss = 465877.81168\n",
      "iteration =  132, loss = 472253.15210\n",
      "iteration =  133, loss = 478725.81717\n",
      "iteration =  134, loss = 485291.62820\n",
      "iteration =  135, loss = 491946.51205\n",
      "iteration =  136, loss = 498686.49886\n",
      "iteration =  137, loss = 505507.71973\n",
      "iteration =  138, loss = 512406.40449\n",
      "iteration =  139, loss = 519378.87956\n",
      "iteration =  140, loss = 526421.56577\n",
      "iteration =  141, loss = 533530.97628\n",
      "iteration =  142, loss = 540703.71454\n",
      "iteration =  143, loss = 547936.47228\n",
      "iteration =  144, loss = 555226.02754\n",
      "iteration =  145, loss = 562569.24275\n",
      "iteration =  146, loss = 569963.06287\n",
      "iteration =  147, loss = 577404.51350\n",
      "iteration =  148, loss = 584890.69913\n",
      "iteration =  149, loss = 592418.80132\n",
      "iteration =  150, loss = 599986.07703\n",
      "iteration =  151, loss = 607589.85686\n",
      "iteration =  152, loss = 615227.54347\n",
      "iteration =  153, loss = 622896.60988\n",
      "iteration =  154, loss = 630594.59794\n",
      "iteration =  155, loss = 638319.11675\n",
      "iteration =  156, loss = 646067.84114\n",
      "iteration =  157, loss = 653838.51018\n",
      "iteration =  158, loss = 661628.92574\n",
      "iteration =  159, loss = 669436.95102\n",
      "iteration =  160, loss = 677260.50920\n",
      "iteration =  161, loss = 685097.58206\n",
      "iteration =  162, loss = 692946.20862\n",
      "iteration =  163, loss = 700804.48384\n",
      "iteration =  164, loss = 708670.55735\n",
      "iteration =  165, loss = 716542.63218\n",
      "iteration =  166, loss = 724418.96356\n",
      "iteration =  167, loss = 732297.85766\n",
      "iteration =  168, loss = 740177.67047\n",
      "iteration =  169, loss = 748056.80661\n",
      "iteration =  170, loss = 755933.71825\n",
      "iteration =  171, loss = 763806.90394\n",
      "iteration =  172, loss = 771674.90759\n",
      "iteration =  173, loss = 779536.31737\n",
      "iteration =  174, loss = 787389.76470\n",
      "iteration =  175, loss = 795233.92325\n",
      "iteration =  176, loss = 803067.50792\n",
      "iteration =  177, loss = 810889.27386\n",
      "iteration =  178, loss = 818698.01559\n",
      "iteration =  179, loss = 826492.56599\n",
      "iteration =  180, loss = 834271.79545\n",
      "iteration =  181, loss = 842034.61094\n",
      "iteration =  182, loss = 849779.95517\n",
      "iteration =  183, loss = 857506.80574\n",
      "iteration =  184, loss = 865214.17429\n",
      "iteration =  185, loss = 872901.10568\n",
      "iteration =  186, loss = 880566.67721\n",
      "iteration =  187, loss = 888209.99785\n",
      "iteration =  188, loss = 895830.20746\n",
      "iteration =  189, loss = 903426.47602\n",
      "iteration =  190, loss = 910998.00296\n",
      "iteration =  191, loss = 918544.01639\n",
      "iteration =  192, loss = 926063.77243\n",
      "iteration =  193, loss = 933556.55452\n",
      "iteration =  194, loss = 941021.67276\n",
      "iteration =  195, loss = 948458.46322\n",
      "iteration =  196, loss = 955866.28735\n",
      "iteration =  197, loss = 963244.53132\n",
      "iteration =  198, loss = 970592.60542\n",
      "iteration =  199, loss = 977909.94346\n",
      "iteration =  200, loss = 985196.00218\n",
      "iteration =  201, loss = 992450.26069\n",
      "iteration =  202, loss = 999672.21987\n",
      "iteration =  203, loss = 1006861.40189\n",
      "iteration =  204, loss = 1014017.34962\n",
      "iteration =  205, loss = 1021139.62610\n",
      "iteration =  206, loss = 1028227.81408\n",
      "iteration =  207, loss = 1035281.51547\n",
      "iteration =  208, loss = 1042300.35087\n",
      "iteration =  209, loss = 1049283.95909\n",
      "iteration =  210, loss = 1056231.99666\n",
      "iteration =  211, loss = 1063144.13743\n",
      "iteration =  212, loss = 1070020.07203\n",
      "iteration =  213, loss = 1076859.50752\n",
      "iteration =  214, loss = 1083662.16691\n",
      "iteration =  215, loss = 1090427.78876\n",
      "iteration =  216, loss = 1097156.12677\n",
      "iteration =  217, loss = 1103846.94935\n",
      "iteration =  218, loss = 1110500.03928\n",
      "iteration =  219, loss = 1117115.19330\n",
      "iteration =  220, loss = 1123692.22172\n",
      "iteration =  221, loss = 1130230.94809\n",
      "iteration =  222, loss = 1136731.20880\n",
      "iteration =  223, loss = 1143192.85277\n",
      "iteration =  224, loss = 1149615.74110\n",
      "iteration =  225, loss = 1155999.74670\n",
      "iteration =  226, loss = 1162344.75402\n",
      "iteration =  227, loss = 1168650.65869\n",
      "iteration =  228, loss = 1174917.36722\n",
      "iteration =  229, loss = 1181144.79669\n",
      "iteration =  230, loss = 1187332.87449\n",
      "iteration =  231, loss = 1193481.53796\n",
      "iteration =  232, loss = 1199590.73417\n",
      "iteration =  233, loss = 1205660.41961\n",
      "iteration =  234, loss = 1211690.55991\n",
      "iteration =  235, loss = 1217681.12960\n",
      "iteration =  236, loss = 1223632.11186\n",
      "iteration =  237, loss = 1229543.49822\n",
      "iteration =  238, loss = 1235415.28835\n",
      "iteration =  239, loss = 1241247.48982\n",
      "iteration =  240, loss = 1247040.11784\n",
      "iteration =  241, loss = 1252793.19505\n",
      "iteration =  242, loss = 1258506.75128\n",
      "iteration =  243, loss = 1264180.82335\n",
      "iteration =  244, loss = 1269815.45484\n",
      "iteration =  245, loss = 1275410.69588\n",
      "iteration =  246, loss = 1280966.60296\n",
      "iteration =  247, loss = 1286483.23870\n",
      "iteration =  248, loss = 1291960.67169\n",
      "iteration =  249, loss = 1297398.97629\n",
      "iteration =  250, loss = 1302798.23242\n",
      "iteration =  251, loss = 1308158.52542\n",
      "iteration =  252, loss = 1313479.94582\n",
      "iteration =  253, loss = 1318762.58923\n",
      "iteration =  254, loss = 1324006.55610\n",
      "iteration =  255, loss = 1329211.95162\n",
      "iteration =  256, loss = 1334378.88553\n",
      "iteration =  257, loss = 1339507.47196\n",
      "iteration =  258, loss = 1344597.82927\n",
      "iteration =  259, loss = 1349650.07992\n",
      "iteration =  260, loss = 1354664.35033\n",
      "iteration =  261, loss = 1359640.77070\n",
      "iteration =  262, loss = 1364579.47492\n",
      "iteration =  263, loss = 1369480.60038\n",
      "iteration =  264, loss = 1374344.28789\n",
      "iteration =  265, loss = 1379170.68152\n",
      "iteration =  266, loss = 1383959.92849\n",
      "iteration =  267, loss = 1388712.17902\n",
      "iteration =  268, loss = 1393427.58625\n",
      "iteration =  269, loss = 1398106.30610\n",
      "iteration =  270, loss = 1402748.49715\n",
      "iteration =  271, loss = 1407354.32055\n",
      "iteration =  272, loss = 1411923.93990\n",
      "iteration =  273, loss = 1416457.52113\n",
      "iteration =  274, loss = 1420955.23244\n",
      "iteration =  275, loss = 1425417.24415\n",
      "iteration =  276, loss = 1429843.72863\n",
      "iteration =  277, loss = 1434234.86022\n",
      "iteration =  278, loss = 1438590.81508\n",
      "iteration =  279, loss = 1442911.77117\n",
      "iteration =  280, loss = 1447197.90812\n",
      "iteration =  281, loss = 1451449.40715\n",
      "iteration =  282, loss = 1455666.45099\n",
      "iteration =  283, loss = 1459849.22380\n",
      "iteration =  284, loss = 1463997.91110\n",
      "iteration =  285, loss = 1468112.69967\n",
      "iteration =  286, loss = 1472193.77749\n",
      "iteration =  287, loss = 1476241.33368\n",
      "iteration =  288, loss = 1480255.55839\n",
      "iteration =  289, loss = 1484236.64277\n",
      "iteration =  290, loss = 1488184.77889\n",
      "iteration =  291, loss = 1492100.15967\n",
      "iteration =  292, loss = 1495982.97880\n",
      "iteration =  293, loss = 1499833.43073\n",
      "iteration =  294, loss = 1503651.71055\n",
      "iteration =  295, loss = 1507438.01396\n",
      "iteration =  296, loss = 1511192.53723\n",
      "iteration =  297, loss = 1514915.47711\n",
      "iteration =  298, loss = 1518607.03078\n",
      "iteration =  299, loss = 1522267.39584\n",
      "iteration =  300, loss = 1525896.77019\n",
      "iteration =  301, loss = 1529495.35206\n",
      "iteration =  302, loss = 1533063.33989\n",
      "iteration =  303, loss = 1536600.93233\n",
      "iteration =  304, loss = 1540108.32816\n",
      "iteration =  305, loss = 1543585.72629\n",
      "iteration =  306, loss = 1547033.32568\n",
      "iteration =  307, loss = 1550451.32529\n",
      "iteration =  308, loss = 1553839.92411\n",
      "iteration =  309, loss = 1557199.32102\n",
      "iteration =  310, loss = 1560529.71484\n",
      "iteration =  311, loss = 1563831.30423\n",
      "iteration =  312, loss = 1567104.28771\n",
      "iteration =  313, loss = 1570348.86356\n",
      "iteration =  314, loss = 1573565.22986\n",
      "iteration =  315, loss = 1576753.58440\n",
      "iteration =  316, loss = 1579914.12468\n",
      "iteration =  317, loss = 1583047.04785\n",
      "iteration =  318, loss = 1586152.55073\n",
      "iteration =  319, loss = 1589230.82972\n",
      "iteration =  320, loss = 1592282.08083\n",
      "iteration =  321, loss = 1595306.49962\n",
      "iteration =  322, loss = 1598304.28116\n",
      "iteration =  323, loss = 1601275.62006\n",
      "iteration =  324, loss = 1604220.71039\n",
      "iteration =  325, loss = 1607139.74569\n",
      "iteration =  326, loss = 1610032.91893\n",
      "iteration =  327, loss = 1612900.42249\n",
      "iteration =  328, loss = 1615742.44817\n",
      "iteration =  329, loss = 1618559.18711\n",
      "iteration =  330, loss = 1621350.82982\n",
      "iteration =  331, loss = 1624117.56615\n",
      "iteration =  332, loss = 1626859.58527\n",
      "iteration =  333, loss = 1629577.07564\n",
      "iteration =  334, loss = 1632270.22501\n",
      "iteration =  335, loss = 1634939.22038\n",
      "iteration =  336, loss = 1637584.24804\n",
      "iteration =  337, loss = 1640205.49348\n",
      "iteration =  338, loss = 1642803.14144\n",
      "iteration =  339, loss = 1645377.37584\n",
      "iteration =  340, loss = 1647928.37983\n",
      "iteration =  341, loss = 1650456.33571\n",
      "iteration =  342, loss = 1652961.42499\n",
      "iteration =  343, loss = 1655443.82830\n",
      "iteration =  344, loss = 1657903.72544\n",
      "iteration =  345, loss = 1660341.29535\n",
      "iteration =  346, loss = 1662756.71610\n",
      "iteration =  347, loss = 1665150.16487\n",
      "iteration =  348, loss = 1667521.81795\n",
      "iteration =  349, loss = 1669871.85073\n",
      "iteration =  350, loss = 1672200.43771\n",
      "iteration =  351, loss = 1674507.75246\n",
      "iteration =  352, loss = 1676793.96762\n",
      "iteration =  353, loss = 1679059.25493\n",
      "iteration =  354, loss = 1681303.78516\n",
      "iteration =  355, loss = 1683527.72816\n",
      "iteration =  356, loss = 1685731.25283\n",
      "iteration =  357, loss = 1687914.52711\n",
      "iteration =  358, loss = 1690077.71799\n",
      "iteration =  359, loss = 1692220.99148\n",
      "iteration =  360, loss = 1694344.51265\n",
      "iteration =  361, loss = 1696448.44558\n",
      "iteration =  362, loss = 1698532.95336\n",
      "iteration =  363, loss = 1700598.19815\n",
      "iteration =  364, loss = 1702644.34107\n",
      "iteration =  365, loss = 1704671.54229\n",
      "iteration =  366, loss = 1706679.96099\n",
      "iteration =  367, loss = 1708669.75535\n",
      "iteration =  368, loss = 1710641.08256\n",
      "iteration =  369, loss = 1712594.09882\n",
      "iteration =  370, loss = 1714528.95934\n",
      "iteration =  371, loss = 1716445.81832\n",
      "iteration =  372, loss = 1718344.82896\n",
      "iteration =  373, loss = 1720226.14349\n",
      "iteration =  374, loss = 1722089.91311\n",
      "iteration =  375, loss = 1723936.28804\n",
      "iteration =  376, loss = 1725765.41747\n",
      "iteration =  377, loss = 1727577.44963\n",
      "iteration =  378, loss = 1729372.53173\n",
      "iteration =  379, loss = 1731150.80998\n",
      "iteration =  380, loss = 1732912.42959\n",
      "iteration =  381, loss = 1734657.53478\n",
      "iteration =  382, loss = 1736386.26876\n",
      "iteration =  383, loss = 1738098.77377\n",
      "iteration =  384, loss = 1739795.19102\n",
      "iteration =  385, loss = 1741475.66077\n",
      "iteration =  386, loss = 1743140.32223\n",
      "iteration =  387, loss = 1744789.31369\n",
      "iteration =  388, loss = 1746422.77238\n",
      "iteration =  389, loss = 1748040.83461\n",
      "iteration =  390, loss = 1749643.63566\n",
      "iteration =  391, loss = 1751231.30985\n",
      "iteration =  392, loss = 1752803.99051\n",
      "iteration =  393, loss = 1754361.81001\n",
      "iteration =  394, loss = 1755904.89973\n",
      "iteration =  395, loss = 1757433.39008\n",
      "iteration =  396, loss = 1758947.41051\n",
      "iteration =  397, loss = 1760447.08950\n",
      "iteration =  398, loss = 1761932.55458\n",
      "iteration =  399, loss = 1763403.93229\n",
      "iteration =  400, loss = 1764861.34824\n",
      "iteration =  401, loss = 1766304.92710\n",
      "iteration =  402, loss = 1767734.79255\n",
      "iteration =  403, loss = 1769151.06736\n",
      "iteration =  404, loss = 1770553.87335\n",
      "iteration =  405, loss = 1771943.33139\n",
      "iteration =  406, loss = 1773319.56143\n",
      "iteration =  407, loss = 1774682.68249\n",
      "iteration =  408, loss = 1776032.81265\n",
      "iteration =  409, loss = 1777370.06909\n",
      "iteration =  410, loss = 1778694.56804\n",
      "iteration =  411, loss = 1780006.42485\n",
      "iteration =  412, loss = 1781305.75393\n",
      "iteration =  413, loss = 1782592.66881\n",
      "iteration =  414, loss = 1783867.28211\n",
      "iteration =  415, loss = 1785129.70555\n",
      "iteration =  416, loss = 1786380.04995\n",
      "iteration =  417, loss = 1787618.42527\n",
      "iteration =  418, loss = 1788844.94054\n",
      "iteration =  419, loss = 1790059.70396\n",
      "iteration =  420, loss = 1791262.82283\n",
      "iteration =  421, loss = 1792454.40357\n",
      "iteration =  422, loss = 1793634.55177\n",
      "iteration =  423, loss = 1794803.37213\n",
      "iteration =  424, loss = 1795960.96849\n",
      "iteration =  425, loss = 1797107.44388\n",
      "iteration =  426, loss = 1798242.90043\n",
      "iteration =  427, loss = 1799367.43946\n",
      "iteration =  428, loss = 1800481.16146\n",
      "iteration =  429, loss = 1801584.16607\n",
      "iteration =  430, loss = 1802676.55210\n",
      "iteration =  431, loss = 1803758.41756\n",
      "iteration =  432, loss = 1804829.85962\n",
      "iteration =  433, loss = 1805890.97466\n",
      "iteration =  434, loss = 1806941.85823\n",
      "iteration =  435, loss = 1807982.60510\n",
      "iteration =  436, loss = 1809013.30922\n",
      "iteration =  437, loss = 1810034.06377\n",
      "iteration =  438, loss = 1811044.96112\n",
      "iteration =  439, loss = 1812046.09288\n",
      "iteration =  440, loss = 1813037.54987\n",
      "iteration =  441, loss = 1814019.42214\n",
      "iteration =  442, loss = 1814991.79897\n",
      "iteration =  443, loss = 1815954.76889\n",
      "iteration =  444, loss = 1816908.41965\n",
      "iteration =  445, loss = 1817852.83828\n",
      "iteration =  446, loss = 1818788.11102\n",
      "iteration =  447, loss = 1819714.32342\n",
      "iteration =  448, loss = 1820631.56024\n",
      "iteration =  449, loss = 1821539.90555\n",
      "iteration =  450, loss = 1822439.44268\n",
      "iteration =  451, loss = 1823330.25421\n",
      "iteration =  452, loss = 1824212.42205\n",
      "iteration =  453, loss = 1825086.02737\n",
      "iteration =  454, loss = 1825951.15063\n",
      "iteration =  455, loss = 1826807.87161\n",
      "iteration =  456, loss = 1827656.26936\n",
      "iteration =  457, loss = 1828496.42227\n",
      "iteration =  458, loss = 1829328.40803\n",
      "iteration =  459, loss = 1830152.30364\n",
      "iteration =  460, loss = 1830968.18544\n",
      "iteration =  461, loss = 1831776.12908\n",
      "iteration =  462, loss = 1832576.20955\n",
      "iteration =  463, loss = 1833368.50118\n",
      "iteration =  464, loss = 1834153.07764\n",
      "iteration =  465, loss = 1834930.01196\n",
      "iteration =  466, loss = 1835699.37648\n",
      "iteration =  467, loss = 1836461.24296\n",
      "iteration =  468, loss = 1837215.68247\n",
      "iteration =  469, loss = 1837962.76546\n",
      "iteration =  470, loss = 1838702.56177\n",
      "iteration =  471, loss = 1839435.14059\n",
      "iteration =  472, loss = 1840160.57052\n",
      "iteration =  473, loss = 1840878.91951\n",
      "iteration =  474, loss = 1841590.25493\n",
      "iteration =  475, loss = 1842294.64354\n",
      "iteration =  476, loss = 1842992.15148\n",
      "iteration =  477, loss = 1843682.84431\n",
      "iteration =  478, loss = 1844366.78701\n",
      "iteration =  479, loss = 1845044.04395\n",
      "iteration =  480, loss = 1845714.67894\n",
      "iteration =  481, loss = 1846378.75519\n",
      "iteration =  482, loss = 1847036.33535\n",
      "iteration =  483, loss = 1847687.48152\n",
      "iteration =  484, loss = 1848332.25519\n",
      "iteration =  485, loss = 1848970.71735\n",
      "iteration =  486, loss = 1849602.92838\n",
      "iteration =  487, loss = 1850228.94814\n",
      "iteration =  488, loss = 1850848.83595\n",
      "iteration =  489, loss = 1851462.65057\n",
      "iteration =  490, loss = 1852070.45023\n",
      "iteration =  491, loss = 1852672.29263\n",
      "iteration =  492, loss = 1853268.23494\n",
      "iteration =  493, loss = 1853858.33380\n",
      "iteration =  494, loss = 1854442.64534\n",
      "iteration =  495, loss = 1855021.22518\n",
      "iteration =  496, loss = 1855594.12841\n",
      "iteration =  497, loss = 1856161.40963\n",
      "iteration =  498, loss = 1856723.12292\n",
      "iteration =  499, loss = 1857279.32188\n",
      "iteration =  500, loss = 1857830.05961\n",
      "iteration =  501, loss = 1858375.38871\n",
      "iteration =  502, loss = 1858915.36130\n",
      "iteration =  503, loss = 1859450.02903\n",
      "iteration =  504, loss = 1859979.44306\n",
      "iteration =  505, loss = 1860503.65406\n",
      "iteration =  506, loss = 1861022.71226\n",
      "iteration =  507, loss = 1861536.66741\n",
      "iteration =  508, loss = 1862045.56880\n",
      "iteration =  509, loss = 1862549.46526\n",
      "iteration =  510, loss = 1863048.40516\n",
      "iteration =  511, loss = 1863542.43643\n",
      "iteration =  512, loss = 1864031.60655\n",
      "iteration =  513, loss = 1864515.96255\n",
      "iteration =  514, loss = 1864995.55104\n",
      "iteration =  515, loss = 1865470.41817\n",
      "iteration =  516, loss = 1865940.60968\n",
      "iteration =  517, loss = 1866406.17086\n",
      "iteration =  518, loss = 1866867.14660\n",
      "iteration =  519, loss = 1867323.58135\n",
      "iteration =  520, loss = 1867775.51916\n",
      "iteration =  521, loss = 1868223.00365\n",
      "iteration =  522, loss = 1868666.07804\n",
      "iteration =  523, loss = 1869104.78515\n",
      "iteration =  524, loss = 1869539.16738\n",
      "iteration =  525, loss = 1869969.26674\n",
      "iteration =  526, loss = 1870395.12486\n",
      "iteration =  527, loss = 1870816.78295\n",
      "iteration =  528, loss = 1871234.28185\n",
      "iteration =  529, loss = 1871647.66202\n",
      "iteration =  530, loss = 1872056.96350\n",
      "iteration =  531, loss = 1872462.22601\n",
      "iteration =  532, loss = 1872863.48885\n",
      "iteration =  533, loss = 1873260.79096\n",
      "iteration =  534, loss = 1873654.17092\n",
      "iteration =  535, loss = 1874043.66694\n",
      "iteration =  536, loss = 1874429.31686\n",
      "iteration =  537, loss = 1874811.15818\n",
      "iteration =  538, loss = 1875189.22803\n",
      "iteration =  539, loss = 1875563.56318\n",
      "iteration =  540, loss = 1875934.20007\n",
      "iteration =  541, loss = 1876301.17479\n",
      "iteration =  542, loss = 1876664.52307\n",
      "iteration =  543, loss = 1877024.28032\n",
      "iteration =  544, loss = 1877380.48161\n",
      "iteration =  545, loss = 1877733.16166\n",
      "iteration =  546, loss = 1878082.35488\n",
      "iteration =  547, loss = 1878428.09534\n",
      "iteration =  548, loss = 1878770.41679\n",
      "iteration =  549, loss = 1879109.35265\n",
      "iteration =  550, loss = 1879444.93603\n",
      "iteration =  551, loss = 1879777.19973\n",
      "iteration =  552, loss = 1880106.17621\n",
      "iteration =  553, loss = 1880431.89765\n",
      "iteration =  554, loss = 1880754.39591\n",
      "iteration =  555, loss = 1881073.70254\n",
      "iteration =  556, loss = 1881389.84879\n",
      "iteration =  557, loss = 1881702.86562\n",
      "iteration =  558, loss = 1882012.78368\n",
      "iteration =  559, loss = 1882319.63334\n",
      "iteration =  560, loss = 1882623.44466\n",
      "iteration =  561, loss = 1882924.24744\n",
      "iteration =  562, loss = 1883222.07116\n",
      "iteration =  563, loss = 1883516.94505\n",
      "iteration =  564, loss = 1883808.89803\n",
      "iteration =  565, loss = 1884097.95876\n",
      "iteration =  566, loss = 1884384.15562\n",
      "iteration =  567, loss = 1884667.51672\n",
      "iteration =  568, loss = 1884948.06989\n",
      "iteration =  569, loss = 1885225.84270\n",
      "iteration =  570, loss = 1885500.86245\n",
      "iteration =  571, loss = 1885773.15619\n",
      "iteration =  572, loss = 1886042.75069\n",
      "iteration =  573, loss = 1886309.67247\n",
      "iteration =  574, loss = 1886573.94780\n",
      "iteration =  575, loss = 1886835.60269\n",
      "iteration =  576, loss = 1887094.66290\n",
      "iteration =  577, loss = 1887351.15394\n",
      "iteration =  578, loss = 1887605.10107\n",
      "iteration =  579, loss = 1887856.52933\n",
      "iteration =  580, loss = 1888105.46348\n",
      "iteration =  581, loss = 1888351.92806\n",
      "iteration =  582, loss = 1888595.94739\n",
      "iteration =  583, loss = 1888837.54552\n",
      "iteration =  584, loss = 1889076.74628\n",
      "iteration =  585, loss = 1889313.57329\n",
      "iteration =  586, loss = 1889548.04992\n",
      "iteration =  587, loss = 1889780.19931\n",
      "iteration =  588, loss = 1890010.04439\n",
      "iteration =  589, loss = 1890237.60785\n",
      "iteration =  590, loss = 1890462.91219\n",
      "iteration =  591, loss = 1890685.97966\n",
      "iteration =  592, loss = 1890906.83231\n",
      "iteration =  593, loss = 1891125.49197\n",
      "iteration =  594, loss = 1891341.98026\n",
      "iteration =  595, loss = 1891556.31860\n",
      "iteration =  596, loss = 1891768.52819\n",
      "iteration =  597, loss = 1891978.63001\n",
      "iteration =  598, loss = 1892186.64487\n",
      "iteration =  599, loss = 1892392.59336\n",
      "iteration =  600, loss = 1892596.49586\n",
      "iteration =  601, loss = 1892798.37256\n",
      "iteration =  602, loss = 1892998.24347\n",
      "iteration =  603, loss = 1893196.12837\n",
      "iteration =  604, loss = 1893392.04689\n",
      "iteration =  605, loss = 1893586.01842\n",
      "iteration =  606, loss = 1893778.06221\n",
      "iteration =  607, loss = 1893968.19729\n",
      "iteration =  608, loss = 1894156.44251\n",
      "iteration =  609, loss = 1894342.81654\n",
      "iteration =  610, loss = 1894527.33787\n",
      "iteration =  611, loss = 1894710.02481\n",
      "iteration =  612, loss = 1894890.89549\n",
      "iteration =  613, loss = 1895069.96786\n",
      "iteration =  614, loss = 1895247.25969\n",
      "iteration =  615, loss = 1895422.78858\n",
      "iteration =  616, loss = 1895596.57198\n",
      "iteration =  617, loss = 1895768.62713\n",
      "iteration =  618, loss = 1895938.97113\n",
      "iteration =  619, loss = 1896107.62091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration =  620, loss = 1896274.59322\n",
      "iteration =  621, loss = 1896439.90466\n",
      "iteration =  622, loss = 1896603.57165\n",
      "iteration =  623, loss = 1896765.61048\n",
      "iteration =  624, loss = 1896926.03726\n",
      "iteration =  625, loss = 1897084.86793\n",
      "iteration =  626, loss = 1897242.11830\n",
      "iteration =  627, loss = 1897397.80402\n",
      "iteration =  628, loss = 1897551.94056\n",
      "iteration =  629, loss = 1897704.54328\n",
      "iteration =  630, loss = 1897855.62735\n",
      "iteration =  631, loss = 1898005.20782\n",
      "iteration =  632, loss = 1898153.29959\n",
      "iteration =  633, loss = 1898299.91739\n",
      "iteration =  634, loss = 1898445.07583\n",
      "iteration =  635, loss = 1898588.78936\n",
      "iteration =  636, loss = 1898731.07230\n",
      "iteration =  637, loss = 1898871.93883\n",
      "iteration =  638, loss = 1899011.40299\n",
      "iteration =  639, loss = 1899149.47867\n",
      "iteration =  640, loss = 1899286.17963\n",
      "iteration =  641, loss = 1899421.51950\n",
      "iteration =  642, loss = 1899555.51178\n",
      "iteration =  643, loss = 1899688.16983\n",
      "iteration =  644, loss = 1899819.50686\n",
      "iteration =  645, loss = 1899949.53599\n",
      "iteration =  646, loss = 1900078.27019\n",
      "iteration =  647, loss = 1900205.72229\n",
      "iteration =  648, loss = 1900331.90501\n",
      "iteration =  649, loss = 1900456.83095\n",
      "iteration =  650, loss = 1900580.51257\n",
      "iteration =  651, loss = 1900702.96221\n",
      "iteration =  652, loss = 1900824.19211\n",
      "iteration =  653, loss = 1900944.21436\n",
      "iteration =  654, loss = 1901063.04094\n",
      "iteration =  655, loss = 1901180.68373\n",
      "iteration =  656, loss = 1901297.15447\n",
      "iteration =  657, loss = 1901412.46480\n",
      "iteration =  658, loss = 1901526.62622\n",
      "iteration =  659, loss = 1901639.65016\n",
      "iteration =  660, loss = 1901751.54790\n",
      "iteration =  661, loss = 1901862.33063\n",
      "iteration =  662, loss = 1901972.00941\n",
      "iteration =  663, loss = 1902080.59520\n",
      "iteration =  664, loss = 1902188.09887\n",
      "iteration =  665, loss = 1902294.53116\n",
      "iteration =  666, loss = 1902399.90270\n",
      "iteration =  667, loss = 1902504.22404\n",
      "iteration =  668, loss = 1902607.50561\n",
      "iteration =  669, loss = 1902709.75774\n",
      "iteration =  670, loss = 1902810.99064\n",
      "iteration =  671, loss = 1902911.21446\n",
      "iteration =  672, loss = 1903010.43922\n",
      "iteration =  673, loss = 1903108.67484\n",
      "iteration =  674, loss = 1903205.93116\n",
      "iteration =  675, loss = 1903302.21790\n",
      "iteration =  676, loss = 1903397.54470\n",
      "iteration =  677, loss = 1903491.92111\n",
      "iteration =  678, loss = 1903585.35657\n",
      "iteration =  679, loss = 1903677.86043\n",
      "iteration =  680, loss = 1903769.44196\n",
      "iteration =  681, loss = 1903860.11032\n",
      "iteration =  682, loss = 1903949.87460\n",
      "iteration =  683, loss = 1904038.74377\n",
      "iteration =  684, loss = 1904126.72675\n",
      "iteration =  685, loss = 1904213.83235\n",
      "iteration =  686, loss = 1904300.06929\n",
      "iteration =  687, loss = 1904385.44620\n",
      "iteration =  688, loss = 1904469.97165\n",
      "iteration =  689, loss = 1904553.65410\n",
      "iteration =  690, loss = 1904636.50194\n",
      "iteration =  691, loss = 1904718.52346\n",
      "iteration =  692, loss = 1904799.72690\n",
      "iteration =  693, loss = 1904880.12038\n",
      "iteration =  694, loss = 1904959.71197\n",
      "iteration =  695, loss = 1905038.50964\n",
      "iteration =  696, loss = 1905116.52129\n",
      "iteration =  697, loss = 1905193.75475\n",
      "iteration =  698, loss = 1905270.21776\n",
      "iteration =  699, loss = 1905345.91798\n",
      "iteration =  700, loss = 1905420.86301\n",
      "iteration =  701, loss = 1905495.06037\n",
      "iteration =  702, loss = 1905568.51748\n",
      "iteration =  703, loss = 1905641.24173\n",
      "iteration =  704, loss = 1905713.24040\n",
      "iteration =  705, loss = 1905784.52073\n",
      "iteration =  706, loss = 1905855.08985\n",
      "iteration =  707, loss = 1905924.95485\n",
      "iteration =  708, loss = 1905994.12274\n",
      "iteration =  709, loss = 1906062.60047\n",
      "iteration =  710, loss = 1906130.39489\n",
      "iteration =  711, loss = 1906197.51282\n",
      "iteration =  712, loss = 1906263.96099\n",
      "iteration =  713, loss = 1906329.74608\n",
      "iteration =  714, loss = 1906394.87467\n",
      "iteration =  715, loss = 1906459.35332\n",
      "iteration =  716, loss = 1906523.18850\n",
      "iteration =  717, loss = 1906586.38660\n",
      "iteration =  718, loss = 1906648.95399\n",
      "iteration =  719, loss = 1906710.89694\n",
      "iteration =  720, loss = 1906772.22166\n",
      "iteration =  721, loss = 1906832.93433\n",
      "iteration =  722, loss = 1906893.04103\n",
      "iteration =  723, loss = 1906952.54780\n",
      "iteration =  724, loss = 1907011.46062\n",
      "iteration =  725, loss = 1907069.78541\n",
      "iteration =  726, loss = 1907127.52802\n",
      "iteration =  727, loss = 1907184.69426\n",
      "iteration =  728, loss = 1907241.28987\n",
      "iteration =  729, loss = 1907297.32053\n",
      "iteration =  730, loss = 1907352.79187\n",
      "iteration =  731, loss = 1907407.70947\n",
      "iteration =  732, loss = 1907462.07884\n",
      "iteration =  733, loss = 1907515.90545\n",
      "iteration =  734, loss = 1907569.19471\n",
      "iteration =  735, loss = 1907621.95197\n",
      "iteration =  736, loss = 1907674.18253\n",
      "iteration =  737, loss = 1907725.89165\n",
      "iteration =  738, loss = 1907777.08452\n",
      "iteration =  739, loss = 1907827.76629\n",
      "iteration =  740, loss = 1907877.94205\n",
      "iteration =  741, loss = 1907927.61684\n",
      "iteration =  742, loss = 1907976.79567\n",
      "iteration =  743, loss = 1908025.48347\n",
      "iteration =  744, loss = 1908073.68513\n",
      "iteration =  745, loss = 1908121.40552\n",
      "iteration =  746, loss = 1908168.64941\n",
      "iteration =  747, loss = 1908215.42157\n",
      "iteration =  748, loss = 1908261.72670\n",
      "iteration =  749, loss = 1908307.56945\n",
      "iteration =  750, loss = 1908352.95444\n",
      "iteration =  751, loss = 1908397.88623\n",
      "iteration =  752, loss = 1908442.36933\n",
      "iteration =  753, loss = 1908486.40823\n",
      "iteration =  754, loss = 1908530.00734\n",
      "iteration =  755, loss = 1908573.17107\n",
      "iteration =  756, loss = 1908615.90374\n",
      "iteration =  757, loss = 1908658.20967\n",
      "iteration =  758, loss = 1908700.09309\n",
      "iteration =  759, loss = 1908741.55824\n",
      "iteration =  760, loss = 1908782.60928\n",
      "iteration =  761, loss = 1908823.25033\n",
      "iteration =  762, loss = 1908863.48550\n",
      "iteration =  763, loss = 1908903.31882\n",
      "iteration =  764, loss = 1908942.75431\n",
      "iteration =  765, loss = 1908981.79594\n",
      "iteration =  766, loss = 1909020.44763\n",
      "iteration =  767, loss = 1909058.71327\n",
      "iteration =  768, loss = 1909096.59671\n",
      "iteration =  769, loss = 1909134.10178\n",
      "iteration =  770, loss = 1909171.23224\n",
      "iteration =  771, loss = 1909207.99182\n",
      "iteration =  772, loss = 1909244.38424\n",
      "iteration =  773, loss = 1909280.41314\n",
      "iteration =  774, loss = 1909316.08217\n",
      "iteration =  775, loss = 1909351.39491\n",
      "iteration =  776, loss = 1909386.35491\n",
      "iteration =  777, loss = 1909420.96570\n",
      "iteration =  778, loss = 1909455.23076\n",
      "iteration =  779, loss = 1909489.15354\n",
      "iteration =  780, loss = 1909522.73745\n",
      "iteration =  781, loss = 1909555.98587\n",
      "iteration =  782, loss = 1909588.90216\n",
      "iteration =  783, loss = 1909621.48963\n",
      "iteration =  784, loss = 1909653.75156\n",
      "iteration =  785, loss = 1909685.69120\n",
      "iteration =  786, loss = 1909717.31176\n",
      "iteration =  787, loss = 1909748.61643\n",
      "iteration =  788, loss = 1909779.60836\n",
      "iteration =  789, loss = 1909810.29067\n",
      "iteration =  790, loss = 1909840.66646\n",
      "iteration =  791, loss = 1909870.73878\n",
      "iteration =  792, loss = 1909900.51066\n",
      "iteration =  793, loss = 1909929.98511\n",
      "iteration =  794, loss = 1909959.16508\n",
      "iteration =  795, loss = 1909988.05351\n",
      "iteration =  796, loss = 1910016.65333\n",
      "iteration =  797, loss = 1910044.96741\n",
      "iteration =  798, loss = 1910072.99860\n",
      "iteration =  799, loss = 1910100.74972\n",
      "iteration =  800, loss = 1910128.22357\n",
      "iteration =  801, loss = 1910155.42293\n",
      "iteration =  802, loss = 1910182.35052\n",
      "iteration =  803, loss = 1910209.00906\n",
      "iteration =  804, loss = 1910235.40125\n",
      "iteration =  805, loss = 1910261.52973\n",
      "iteration =  806, loss = 1910287.39714\n",
      "iteration =  807, loss = 1910313.00609\n",
      "iteration =  808, loss = 1910338.35915\n",
      "iteration =  809, loss = 1910363.45888\n",
      "iteration =  810, loss = 1910388.30782\n",
      "iteration =  811, loss = 1910412.90846\n",
      "iteration =  812, loss = 1910437.26329\n",
      "iteration =  813, loss = 1910461.37475\n",
      "iteration =  814, loss = 1910485.24528\n",
      "iteration =  815, loss = 1910508.87729\n",
      "iteration =  816, loss = 1910532.27315\n",
      "iteration =  817, loss = 1910555.43522\n",
      "iteration =  818, loss = 1910578.36584\n",
      "iteration =  819, loss = 1910601.06732\n",
      "iteration =  820, loss = 1910623.54195\n",
      "iteration =  821, loss = 1910645.79200\n",
      "iteration =  822, loss = 1910667.81969\n",
      "iteration =  823, loss = 1910689.62726\n",
      "iteration =  824, loss = 1910711.21691\n",
      "iteration =  825, loss = 1910732.59081\n",
      "iteration =  826, loss = 1910753.75111\n",
      "iteration =  827, loss = 1910774.69995\n",
      "iteration =  828, loss = 1910795.43944\n",
      "iteration =  829, loss = 1910815.97167\n",
      "iteration =  830, loss = 1910836.29871\n",
      "iteration =  831, loss = 1910856.42260\n",
      "iteration =  832, loss = 1910876.34539\n",
      "iteration =  833, loss = 1910896.06908\n",
      "iteration =  834, loss = 1910915.59565\n",
      "iteration =  835, loss = 1910934.92707\n",
      "iteration =  836, loss = 1910954.06530\n",
      "iteration =  837, loss = 1910973.01226\n",
      "iteration =  838, loss = 1910991.76986\n",
      "iteration =  839, loss = 1911010.34000\n",
      "iteration =  840, loss = 1911028.72454\n",
      "iteration =  841, loss = 1911046.92535\n",
      "iteration =  842, loss = 1911064.94425\n",
      "iteration =  843, loss = 1911082.78307\n",
      "iteration =  844, loss = 1911100.44359\n",
      "iteration =  845, loss = 1911117.92761\n",
      "iteration =  846, loss = 1911135.23689\n",
      "iteration =  847, loss = 1911152.37317\n",
      "iteration =  848, loss = 1911169.33817\n",
      "iteration =  849, loss = 1911186.13362\n",
      "iteration =  850, loss = 1911202.76120\n",
      "iteration =  851, loss = 1911219.22259\n",
      "iteration =  852, loss = 1911235.51946\n",
      "iteration =  853, loss = 1911251.65344\n",
      "iteration =  854, loss = 1911267.62616\n",
      "iteration =  855, loss = 1911283.43923\n",
      "iteration =  856, loss = 1911299.09425\n",
      "iteration =  857, loss = 1911314.59280\n",
      "iteration =  858, loss = 1911329.93644\n",
      "iteration =  859, loss = 1911345.12672\n",
      "iteration =  860, loss = 1911360.16516\n",
      "iteration =  861, loss = 1911375.05330\n",
      "iteration =  862, loss = 1911389.79262\n",
      "iteration =  863, loss = 1911404.38461\n",
      "iteration =  864, loss = 1911418.83076\n",
      "iteration =  865, loss = 1911433.13250\n",
      "iteration =  866, loss = 1911447.29130\n",
      "iteration =  867, loss = 1911461.30857\n",
      "iteration =  868, loss = 1911475.18572\n",
      "iteration =  869, loss = 1911488.92417\n",
      "iteration =  870, loss = 1911502.52529\n",
      "iteration =  871, loss = 1911515.99046\n",
      "iteration =  872, loss = 1911529.32104\n",
      "iteration =  873, loss = 1911542.51836\n",
      "iteration =  874, loss = 1911555.58377\n",
      "iteration =  875, loss = 1911568.51857\n",
      "iteration =  876, loss = 1911581.32408\n",
      "iteration =  877, loss = 1911594.00159\n",
      "iteration =  878, loss = 1911606.55237\n",
      "iteration =  879, loss = 1911618.97769\n",
      "iteration =  880, loss = 1911631.27881\n",
      "iteration =  881, loss = 1911643.45697\n",
      "iteration =  882, loss = 1911655.51339\n",
      "iteration =  883, loss = 1911667.44929\n",
      "iteration =  884, loss = 1911679.26588\n",
      "iteration =  885, loss = 1911690.96434\n",
      "iteration =  886, loss = 1911702.54587\n",
      "iteration =  887, loss = 1911714.01162\n",
      "iteration =  888, loss = 1911725.36275\n",
      "iteration =  889, loss = 1911736.60042\n",
      "iteration =  890, loss = 1911747.72574\n",
      "iteration =  891, loss = 1911758.73986\n",
      "iteration =  892, loss = 1911769.64387\n",
      "iteration =  893, loss = 1911780.43887\n",
      "iteration =  894, loss = 1911791.12597\n",
      "iteration =  895, loss = 1911801.70622\n",
      "iteration =  896, loss = 1911812.18072\n",
      "iteration =  897, loss = 1911822.55050\n",
      "iteration =  898, loss = 1911832.81661\n",
      "iteration =  899, loss = 1911842.98010\n",
      "iteration =  900, loss = 1911853.04199\n",
      "iteration =  901, loss = 1911863.00329\n",
      "iteration =  902, loss = 1911872.86500\n",
      "iteration =  903, loss = 1911882.62813\n",
      "iteration =  904, loss = 1911892.29366\n",
      "iteration =  905, loss = 1911901.86257\n",
      "iteration =  906, loss = 1911911.33581\n",
      "iteration =  907, loss = 1911920.71435\n",
      "iteration =  908, loss = 1911929.99913\n",
      "iteration =  909, loss = 1911939.19109\n",
      "iteration =  910, loss = 1911948.29116\n",
      "iteration =  911, loss = 1911957.30025\n",
      "iteration =  912, loss = 1911966.21928\n",
      "iteration =  913, loss = 1911975.04914\n",
      "iteration =  914, loss = 1911983.79073\n",
      "iteration =  915, loss = 1911992.44492\n",
      "iteration =  916, loss = 1912001.01260\n",
      "iteration =  917, loss = 1912009.49463\n",
      "iteration =  918, loss = 1912017.89185\n",
      "iteration =  919, loss = 1912026.20513\n",
      "iteration =  920, loss = 1912034.43529\n",
      "iteration =  921, loss = 1912042.58318\n",
      "iteration =  922, loss = 1912050.64960\n",
      "iteration =  923, loss = 1912058.63539\n",
      "iteration =  924, loss = 1912066.54133\n",
      "iteration =  925, loss = 1912074.36824\n",
      "iteration =  926, loss = 1912082.11689\n",
      "iteration =  927, loss = 1912089.78808\n",
      "iteration =  928, loss = 1912097.38258\n",
      "iteration =  929, loss = 1912104.90114\n",
      "iteration =  930, loss = 1912112.34454\n",
      "iteration =  931, loss = 1912119.71353\n",
      "iteration =  932, loss = 1912127.00884\n",
      "iteration =  933, loss = 1912134.23121\n",
      "iteration =  934, loss = 1912141.38138\n",
      "iteration =  935, loss = 1912148.46006\n",
      "iteration =  936, loss = 1912155.46797\n",
      "iteration =  937, loss = 1912162.40581\n",
      "iteration =  938, loss = 1912169.27430\n",
      "iteration =  939, loss = 1912176.07411\n",
      "iteration =  940, loss = 1912182.80594\n",
      "iteration =  941, loss = 1912189.47047\n",
      "iteration =  942, loss = 1912196.06836\n",
      "iteration =  943, loss = 1912202.60029\n",
      "iteration =  944, loss = 1912209.06691\n",
      "iteration =  945, loss = 1912215.46888\n",
      "iteration =  946, loss = 1912221.80684\n",
      "iteration =  947, loss = 1912228.08144\n",
      "iteration =  948, loss = 1912234.29330\n",
      "iteration =  949, loss = 1912240.44306\n",
      "iteration =  950, loss = 1912246.53133\n",
      "iteration =  951, loss = 1912252.55873\n",
      "iteration =  952, loss = 1912258.52586\n",
      "iteration =  953, loss = 1912264.43334\n",
      "iteration =  954, loss = 1912270.28175\n",
      "iteration =  955, loss = 1912276.07169\n",
      "iteration =  956, loss = 1912281.80374\n",
      "iteration =  957, loss = 1912287.47848\n",
      "iteration =  958, loss = 1912293.09649\n",
      "iteration =  959, loss = 1912298.65832\n",
      "iteration =  960, loss = 1912304.16454\n",
      "iteration =  961, loss = 1912309.61571\n",
      "iteration =  962, loss = 1912315.01238\n",
      "iteration =  963, loss = 1912320.35509\n",
      "iteration =  964, loss = 1912325.64439\n",
      "iteration =  965, loss = 1912330.88080\n",
      "iteration =  966, loss = 1912336.06485\n",
      "iteration =  967, loss = 1912341.19707\n",
      "iteration =  968, loss = 1912346.27798\n",
      "iteration =  969, loss = 1912351.30809\n",
      "iteration =  970, loss = 1912356.28790\n",
      "iteration =  971, loss = 1912361.21792\n",
      "iteration =  972, loss = 1912366.09865\n",
      "iteration =  973, loss = 1912370.93059\n",
      "iteration =  974, loss = 1912375.71420\n",
      "iteration =  975, loss = 1912380.44999\n",
      "iteration =  976, loss = 1912385.13843\n",
      "iteration =  977, loss = 1912389.78000\n",
      "iteration =  978, loss = 1912394.37515\n",
      "iteration =  979, loss = 1912398.92436\n",
      "iteration =  980, loss = 1912403.42808\n",
      "iteration =  981, loss = 1912407.88677\n",
      "iteration =  982, loss = 1912412.30088\n",
      "iteration =  983, loss = 1912416.67086\n",
      "iteration =  984, loss = 1912420.99714\n",
      "iteration =  985, loss = 1912425.28016\n",
      "iteration =  986, loss = 1912429.52036\n",
      "iteration =  987, loss = 1912433.71817\n",
      "iteration =  988, loss = 1912437.87400\n",
      "iteration =  989, loss = 1912441.98828\n",
      "iteration =  990, loss = 1912446.06142\n",
      "iteration =  991, loss = 1912450.09383\n",
      "iteration =  992, loss = 1912454.08593\n",
      "iteration =  993, loss = 1912458.03811\n",
      "iteration =  994, loss = 1912461.95077\n",
      "iteration =  995, loss = 1912465.82432\n",
      "iteration =  996, loss = 1912469.65913\n",
      "iteration =  997, loss = 1912473.45559\n",
      "iteration =  998, loss = 1912477.21410\n",
      "iteration =  999, loss = 1912480.93503\n"
     ]
    }
   ],
   "source": [
    "num_iteration       = 1000\n",
    "learning_rate       = 0.01\n",
    "\n",
    "theta0              = 0\n",
    "theta1              = 0\n",
    "\n",
    "theta0_iteration    = np.zeros(num_iteration)\n",
    "theta1_iteration    = np.zeros(num_iteration)\n",
    "loss_iteration      = np.zeros(num_iteration)\n",
    "\n",
    "for i in range(num_iteration):\n",
    "    theta0  = theta0 - learning_rate * compute_gradient_theta0(x, y, theta0, theta1)\n",
    "    theta1  = theta1 - learning_rate * compute_gradient_theta1(x, y, theta0, theta1)\n",
    "    loss    = compute_loss(x, y, theta0, theta1)\n",
    "\n",
    "    theta0_iteration[i] = theta0\n",
    "    theta1_iteration[i] = theta1\n",
    "    loss_iteration[i]   = loss\n",
    "\n",
    "    print(\"iteration = %4d, loss = %5.5f\" % (i, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_regression(x, y, f):\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('linear regression result')\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(loss_iteration):\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('loss curve')\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_model_parameter(theta0_iteration, theta1_iteration):\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('model parameter')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X0  = np.arange(-10, 10, 0.1)\n",
    "X1  = np.arange(-10, 10, 0.1)\n",
    "\n",
    "grid_theta0, grid_theta1 = \n",
    "\n",
    "grid_loss   = \n",
    "\n",
    "\n",
    "def plot_loss_surface(grid_theta0, grid_theta1, grid_loss):\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.title('loss surface')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 01. plot the input data in blue point and the regression result in red curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_regression(x, y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 02. plot the values of the model parameters $\\theta_0$ in blue curve and $\\theta_1$ in green curve over the gradient descent iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_parameter(theta0_iteration, theta1_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 03. plot the loss values $\\mathcal{L}(\\theta)$ in red curve over the gradient descent iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curve(loss_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 04. plot the loss surface in 3-dimension surface where $x$-axis represents $\\theta_0$, $y$-axis represents $\\theta_1$ and $z$-axis represents $\\mathcal{L}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_surface(grid_theta0, grid_theta1, grid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
